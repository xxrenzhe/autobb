/**
 * ä½¿ç”¨ Vertex AI è°ƒç”¨ Gemini API
 * ä¼˜åŠ¿ï¼š
 * 1. æ— éœ€ä»£ç†ï¼Œç›´æ¥é€šè¿‡ Service Account è®¤è¯
 * 2. ä¼ä¸šçº§ç¨³å®šæ€§å’Œ SLA
 * 3. æ›´å¥½çš„é”™è¯¯å¤„ç†å’Œé‡è¯•æœºåˆ¶
 */

import { VertexAI, GenerativeModel, HarmCategory, HarmBlockThreshold } from '@google-cloud/vertexai'
import * as path from 'path'

// é…ç½®
const GCP_PROJECT_ID = process.env.GCP_PROJECT_ID || 'gen-lang-client-0944935873'
const GCP_LOCATION = process.env.GCP_LOCATION || 'us-central1'
const GCP_CREDENTIALS_PATH = process.env.GOOGLE_APPLICATION_CREDENTIALS ||
  path.join(process.cwd(), 'docs/secrets/gcp_autoads_dev.json')

// å•ä¾‹ VertexAI å®¢æˆ·ç«¯
let vertexAI: VertexAI | null = null

/**
 * è·å– VertexAI å®¢æˆ·ç«¯ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰
 */
function getVertexAI(): VertexAI {
  if (!vertexAI) {
    // è®¾ç½®ç¯å¢ƒå˜é‡ä»¥ä½¿ç”¨ Service Account
    if (!process.env.GOOGLE_APPLICATION_CREDENTIALS) {
      process.env.GOOGLE_APPLICATION_CREDENTIALS = GCP_CREDENTIALS_PATH
    }

    console.log(`ğŸ”§ åˆå§‹åŒ– Vertex AI å®¢æˆ·ç«¯...`)
    console.log(`   Project: ${GCP_PROJECT_ID}`)
    console.log(`   Location: ${GCP_LOCATION}`)

    vertexAI = new VertexAI({
      project: GCP_PROJECT_ID,
      location: GCP_LOCATION,
    })

    console.log('âœ“ Vertex AI å®¢æˆ·ç«¯åˆå§‹åŒ–æˆåŠŸ')
  }

  return vertexAI
}

/**
 * è·å–ç”Ÿæˆæ¨¡å‹
 */
function getGenerativeModel(modelName: string): GenerativeModel {
  const client = getVertexAI()

  return client.getGenerativeModel({
    model: modelName,
    safetySettings: [
      {
        category: HarmCategory.HARM_CATEGORY_HATE_SPEECH,
        threshold: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
      },
      {
        category: HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT,
        threshold: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
      },
      {
        category: HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT,
        threshold: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
      },
      {
        category: HarmCategory.HARM_CATEGORY_HARASSMENT,
        threshold: HarmBlockThreshold.BLOCK_MEDIUM_AND_ABOVE,
      },
    ],
  })
}

/**
 * å¸¦é‡è¯•çš„å»¶è¿Ÿå‡½æ•°
 */
async function delay(ms: number): Promise<void> {
  return new Promise(resolve => setTimeout(resolve, ms))
}

/**
 * è°ƒç”¨ Vertex AI Gemini API ç”Ÿæˆå†…å®¹ï¼ˆå¸¦è‡ªåŠ¨é™çº§å’Œé‡è¯•ï¼‰
 *
 * @param params - ç”Ÿæˆå‚æ•°
 * @param params.model - æ¨¡å‹åç§°ï¼Œé»˜è®¤ 'gemini-2.5-pro'
 *   æ”¯æŒçš„æ¨¡å‹ï¼š
 *   - gemini-2.5-pro (ç¨³å®šç‰ˆï¼Œæ¨è)
 *   - gemini-2.5-flash (å¿«é€Ÿç‰ˆ)
 *   - gemini-2.5-flash-lite (è½»é‡ç‰ˆ)
 *   - gemini-3-pro-preview-11-2025 (é¢„è§ˆç‰ˆï¼Œæœ€æ–°)
 * @param params.prompt - æç¤ºè¯
 * @param params.temperature - æ¸©åº¦å‚æ•°ï¼Œé»˜è®¤ 0.7
 * @param params.maxOutputTokens - æœ€å¤§è¾“å‡ºtokensï¼Œé»˜è®¤ 2048
 * @returns ç”Ÿæˆçš„æ–‡æœ¬å†…å®¹
 */
export async function generateContent(params: {
  model?: string
  prompt: string
  temperature?: number
  maxOutputTokens?: number
}): Promise<string> {
  const {
    model = 'gemini-2.5-pro',
    prompt,
    temperature = 0.7,
    maxOutputTokens = 2048,
  } = params

  const maxRetries = 3
  let lastError: Error | null = null

  // å°è¯•ä¸»æ¨¡å‹
  for (let attempt = 1; attempt <= maxRetries; attempt++) {
    try {
      console.log(`ğŸ¤– è°ƒç”¨ Vertex AI: ${model} (å°è¯• ${attempt}/${maxRetries})`)

      const generativeModel = getGenerativeModel(model)

      const result = await generativeModel.generateContent({
        contents: [
          {
            role: 'user',
            parts: [{ text: prompt }],
          },
        ],
        generationConfig: {
          temperature,
          maxOutputTokens,
        },
      })

      const response = result.response

      if (!response.candidates || response.candidates.length === 0) {
        throw new Error('Vertex AI è¿”å›äº†ç©ºå“åº”')
      }

      const candidate = response.candidates[0]

      // æ£€æŸ¥finishReasonä»¥è¯Šæ–­æˆªæ–­é—®é¢˜
      if (candidate.finishReason && candidate.finishReason !== 'STOP') {
        console.warn(`âš ï¸ Vertex AI è¾“å‡ºè¢«æˆªæ–­: ${candidate.finishReason}`)
        if (candidate.finishReason === 'MAX_TOKENS') {
          console.warn(`   åŸå› : è¾¾åˆ°maxOutputTokensé™åˆ¶ (å½“å‰: ${maxOutputTokens})`)
        } else if (candidate.finishReason === 'SAFETY') {
          console.warn(`   åŸå› : å®‰å…¨è¿‡æ»¤è§¦å‘`)
          if (candidate.safetyRatings) {
            console.warn(`   å®‰å…¨è¯„çº§:`, JSON.stringify(candidate.safetyRatings))
          }
        }
      }

      if (!candidate.content || !candidate.content.parts || candidate.content.parts.length === 0) {
        throw new Error('Vertex AI å“åº”ä¸­æ²¡æœ‰å†…å®¹')
      }

      const text = candidate.content.parts
        .map(part => part.text || '')
        .join('')

      if (!text) {
        throw new Error('Vertex AI è¿”å›äº†ç©ºæ–‡æœ¬')
      }

      console.log(`âœ“ Vertex AI è°ƒç”¨æˆåŠŸï¼Œè¿”å› ${text.length} å­—ç¬¦`)

      // è®°å½•tokenä½¿ç”¨æƒ…å†µ
      if (response.usageMetadata) {
        console.log(`   Tokenä½¿ç”¨: prompt=${response.usageMetadata.promptTokenCount}, ` +
          `output=${response.usageMetadata.candidatesTokenCount}, ` +
          `total=${response.usageMetadata.totalTokenCount}`)
      }

      return text
    } catch (error: any) {
      lastError = error
      console.warn(`âš ï¸ Vertex AI è°ƒç”¨å¤±è´¥ (å°è¯• ${attempt}/${maxRetries}): ${error.message}`)

      // æ£€æŸ¥æ˜¯å¦æ˜¯å¯é‡è¯•çš„é”™è¯¯
      const isRetryable =
        error.message?.includes('503') ||
        error.message?.includes('overload') ||
        error.message?.includes('RESOURCE_EXHAUSTED') ||
        error.message?.includes('UNAVAILABLE') ||
        error.message?.includes('DEADLINE_EXCEEDED') ||
        error.message?.includes('timeout') ||
        error.code === 'ECONNRESET' ||
        error.code === 'ETIMEDOUT'

      if (isRetryable && attempt < maxRetries) {
        const waitTime = Math.pow(2, attempt) * 1000 // æŒ‡æ•°é€€é¿: 2s, 4s, 8s
        console.log(`   ç­‰å¾… ${waitTime / 1000}s åé‡è¯•...`)
        await delay(waitTime)
        continue
      }

      // ä¸å¯é‡è¯•æˆ–å·²ç”¨å®Œé‡è¯•æ¬¡æ•°
      break
    }
  }

  // ä¸»æ¨¡å‹å¤±è´¥ï¼Œå°è¯•é™çº§åˆ° flash æ¨¡å‹
  if (model.includes('pro')) {
    const fallbackModel = 'gemini-2.5-flash'
    console.warn(`âš ï¸ ${model} è°ƒç”¨å¤±è´¥ï¼Œé™çº§åˆ° ${fallbackModel}`)

    for (let attempt = 1; attempt <= maxRetries; attempt++) {
      try {
        console.log(`ğŸ¤– è°ƒç”¨ Vertex AI (é™çº§): ${fallbackModel} (å°è¯• ${attempt}/${maxRetries})`)

        const generativeModel = getGenerativeModel(fallbackModel)

        const result = await generativeModel.generateContent({
          contents: [
            {
              role: 'user',
              parts: [{ text: prompt }],
            },
          ],
          generationConfig: {
            temperature,
            maxOutputTokens,
          },
        })

        const response = result.response

        if (!response.candidates || response.candidates.length === 0) {
          throw new Error('Vertex AI (fallback) è¿”å›äº†ç©ºå“åº”')
        }

        const candidate = response.candidates[0]

        if (!candidate.content || !candidate.content.parts || candidate.content.parts.length === 0) {
          throw new Error('Vertex AI (fallback) å“åº”ä¸­æ²¡æœ‰å†…å®¹')
        }

        const text = candidate.content.parts
          .map(part => part.text || '')
          .join('')

        if (!text) {
          throw new Error('Vertex AI (fallback) è¿”å›äº†ç©ºæ–‡æœ¬')
        }

        console.log(`âœ“ Vertex AI (fallback: ${fallbackModel}) è°ƒç”¨æˆåŠŸï¼Œè¿”å› ${text.length} å­—ç¬¦`)

        return text
      } catch (fallbackError: any) {
        console.warn(`âš ï¸ Vertex AI (fallback) è°ƒç”¨å¤±è´¥ (å°è¯• ${attempt}/${maxRetries}): ${fallbackError.message}`)

        if (attempt < maxRetries) {
          const waitTime = Math.pow(2, attempt) * 1000
          console.log(`   ç­‰å¾… ${waitTime / 1000}s åé‡è¯•...`)
          await delay(waitTime)
          continue
        }

        // é™çº§æ¨¡å‹ä¹Ÿå¤±è´¥
        throw new Error(
          `Vertex AI è°ƒç”¨å¤±è´¥ã€‚ä¸»æ¨¡å‹(${model})é”™è¯¯: ${lastError?.message}ã€‚` +
          `é™çº§æ¨¡å‹(${fallbackModel})é”™è¯¯: ${fallbackError.message}`
        )
      }
    }
  }

  // æ‰€æœ‰å°è¯•éƒ½å¤±è´¥
  throw new Error(`Vertex AI è°ƒç”¨å¤±è´¥: ${lastError?.message}`)
}

/**
 * æ£€æŸ¥ Vertex AI è¿æ¥çŠ¶æ€
 */
export async function checkVertexAIConnection(): Promise<boolean> {
  try {
    const model = getGenerativeModel('gemini-2.5-flash')
    const result = await model.generateContent({
      contents: [
        {
          role: 'user',
          parts: [{ text: 'Hello' }],
        },
      ],
      generationConfig: {
        maxOutputTokens: 10,
      },
    })

    return !!(result.response.candidates && result.response.candidates.length > 0)
  } catch (error) {
    console.error('Vertex AI è¿æ¥æ£€æŸ¥å¤±è´¥:', error)
    return false
  }
}
